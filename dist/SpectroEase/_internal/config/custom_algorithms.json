{
  "preprocessing": [
    {
      "name": "Spectral Preprocessor (Baseline+SG+MSC)",
      "code": "import pandas as pd\nimport numpy as np\nfrom typing import Dict, Any\nfrom scipy.signal import savgol_filter\nfrom interfaces.preprocessing_algorithm import PreprocessingAlgorithm\n\n# Data Format Assumptions:\n# - Input data: pd.DataFrame where rows = samples, columns = features/wavelengths\n# - Output data: pd.DataFrame with SAME shape, SAME index, SAME columns\n# - The algorithm performs three sequential preprocessing steps:\n#   1. Baseline correction (linear fit subtraction)\n#   2. Savitzky-Golay filtering (1st derivative)\n#   3. Multiplicative Scatter Correction (MSC)\n#\n# Original Implementation:\n# - Original MATLAB code uses columns = samples, rows = wavelengths\n# - Adapted by transposing data internally for calculations, then transposing back\n# - MATLAB polyfit returns [slope, intercept] → numpy.polyfit returns [slope, intercept]\n# - MATLAB MSC: (y - p(2)) / p(1) where p(1)=slope, p(2)=intercept\n# - Python MSC: (y - p[1]) / p[0] where p[0]=slope, p[1]=intercept (numpy.polyfit)\n# - Numerical stability checks added for MSC division operations\n\nclass SpectralPreprocessor(PreprocessingAlgorithm):\n    def get_name(self) -> str:\n        return \"Spectral Preprocessor (Baseline+SG+MSC)\"\n    \n    def get_params_info(self) -> Dict[str, Any]:\n        return {\n            'wavelengths': {\n                'type': 'list',\n                'default': [],\n                'description': 'List of wavelength values corresponding to columns (features). Required parameter.'\n            },\n            'sg_polyorder': {\n                'type': 'int',\n                'default': 2,\n                'description': 'Savitzky-Golay polynomial order (must be less than window_length)'\n            },\n            'sg_window_length': {\n                'type': 'int',\n                'default': 15,\n                'description': 'Savitzky-Golay window length (must be odd and positive)'\n            },\n            'sg_deriv': {\n                'type': 'int',\n                'default': 1,\n                'description': 'Savitzky-Golay derivative order (0=smoothing, 1=1st derivative, 2=2nd derivative)'\n            }\n        }\n    \n    def apply(self, data: pd.DataFrame, params: Dict) -> pd.DataFrame:\n        # Store original index and columns for preservation\n        original_index = data.index\n        original_columns = data.columns\n        \n        # Extract parameters\n        wavelengths = params.get('wavelengths', [])\n        sg_polyorder = params.get('sg_polyorder', 2)\n        sg_window_length = params.get('sg_window_length', 15)\n        sg_deriv = params.get('sg_deriv', 1)\n        \n        # Validate wavelengths parameter\n        if not wavelengths:\n            raise ValueError(\"'wavelengths' parameter is required and cannot be empty\")\n        \n        if len(wavelengths) != data.shape[1]:\n            raise ValueError(f\"Length of wavelengths ({len(wavelengths)}) must match number of features ({data.shape[1]})\")\n        \n        # Convert to numpy array and transpose to match original MATLAB orientation\n        # Original: rows = wavelengths, columns = samples\n        # System: rows = samples, columns = wavelengths\n        # So we need to transpose for internal calculations\n        X = data.values.T  # Now shape: (n_features, n_samples)\n        \n        # Verify shape matches wavelengths length\n        if X.shape[0] != len(wavelengths):\n            raise ValueError(f\"Number of rows in transposed data ({X.shape[0]}) != length of wavelengths ({len(wavelengths)})\")\n        \n        n_samples = X.shape[1]\n        \n        # 1) Baseline correction\n        X_bc = np.zeros_like(X)\n        \n        for i in range(n_samples):\n            y = X[:, i]\n            # numpy.polyfit returns [slope, intercept] (same as MATLAB)\n            p = np.polyfit(wavelengths, y, 1)\n            yb = np.polyval(p, wavelengths)\n            X_bc[:, i] = y - yb\n        \n        # 2) SG filter (1st derivative as in original code)\n        X_sg = np.zeros_like(X_bc)\n        \n        for i in range(n_samples):\n            # Apply Savitzky-Golay filter with specified parameters\n            # MATLAB: sgolayfilt(data, polyorder, window_length, deriv)\n            # Python: savgol_filter(data, window_length, polyorder, deriv=deriv)\n            X_sg[:, i] = savgol_filter(\n                X_bc[:, i], \n                window_length=sg_window_length, \n                polyorder=sg_polyorder, \n                deriv=sg_deriv\n            )\n        \n        # 3) MSC (Multiplicative Scatter Correction)\n        # Calculate reference spectrum (mean across samples)\n        # axis=1: mean across columns (samples) → shape: (n_features,)\n        m_ref = np.mean(X_sg, axis=1)\n        \n        X_msc = np.zeros_like(X_sg)\n        \n        for i in range(n_samples):\n            y = X_sg[:, i]\n            # numpy.polyfit returns [slope, intercept] (same as MATLAB)\n            p = np.polyfit(m_ref, y, 1)\n            \n            # MATLAB: (y - p(2)) / p(1) where p(1)=slope, p(2)=intercept\n            # Python: (y - p[1]) / p[0] where p[0]=slope, p[1]=intercept\n            \n            # MANDATORY NUMERICAL STABILITY CHECK for division\n            slope = p[0]\n            intercept = p[1]\n            \n            if abs(slope) < 1e-8:\n                # If slope is near zero, use original spectrum (skip MSC for this sample)\n                X_msc[:, i] = y\n            else:\n                X_msc[:, i] = (y - intercept) / slope\n        \n        # Transpose back to system format: rows = samples, columns = features\n        result_array = X_msc.T  # Shape: (n_samples, n_features)\n        \n        # Verify shape matches original input\n        assert result_array.shape == data.shape, \\\n            f\"Output shape {result_array.shape} doesn't match input shape {data.shape}\"\n        \n        # Return DataFrame with preserved index and columns\n        return pd.DataFrame(result_array, index=original_index, columns=original_columns)",
      "created_at": "2025-12-11T16:14:40.647106",
      "updated_at": "2025-12-11T16:14:40.647106"
    }
  ],
  "feature_selection": [
    {
      "name": "PCA Loading Feature Selector",
      "code": "import pandas as pd\nimport numpy as np\nfrom typing import Dict, Any, List\n\n# Data Format Assumptions:\n# - Input X: rows = samples, columns = features/wavelengths\n# - Input y: Series with one target value per sample (same length as X)\n# - Output: List[str] of selected feature names from X.columns\n#\n# Original Implementation:\n# - Original C++ code reads CSV where rows = samples, columns = features\n# - It transposes to get X with shape (n_features, n_samples)\n# - Computes covariance of features: Cov = (X @ X.T) / (n_samples - 1)\n# - Performs eigen decomposition on feature covariance matrix\n# - Selects features with largest absolute loadings from eigenvector with largest eigenvalue\n# - Returns 1-based indices (converted to 0-based for Python column indexing)\n# - Selects top k features (default k=10)\n#\n# Adaptation Notes:\n# - X input is DataFrame with shape (n_samples, n_features)\n# - Transpose internally to match original layout: X_array.T shape (n_features, n_samples)\n# - Map selected indices back to X.columns names\n# - Return column names, not indices\n\nclass PCALoadingFeatureSelector(FeatureSelectionAlgorithm):\n    def get_name(self) -> str:\n        return \"PCA Loading Feature Selector\"\n    \n    def get_params_info(self) -> Dict[str, Any]:\n        return {\n            'n_features': {\n                'type': 'int',\n                'default': 10,\n                'description': 'Number of features to select based on largest absolute PCA loadings'\n            }\n        }\n    \n    def select_features(self, X: pd.DataFrame, y: pd.Series, params: Dict) -> List[str]:\n        # Extract parameter\n        n_features = params.get('n_features', 10)\n        \n        # Validate input\n        if X.empty:\n            return []\n        \n        # Get original index and columns for preservation\n        original_columns = X.columns\n        \n        # Convert to numpy array and transpose to match original algorithm layout\n        # Original: X shape after transpose = (n_features, n_samples)\n        # Our X: shape = (n_samples, n_features)\n        # So we transpose: X_array.T shape = (n_features, n_samples)\n        X_array = X.values\n        n_samples, n_features_total = X_array.shape\n        \n        # Numerical stability check: need at least 2 samples for covariance\n        if n_samples < 2:\n            # Fallback: return first n_features columns\n            n_selected = min(n_features, n_features_total)\n            return original_columns[:n_selected].tolist()\n        \n        # Transpose to match original algorithm: features × samples\n        X_transposed = X_array.T  # shape: (n_features_total, n_samples)\n        \n        # Center the data: subtract column means (mean across samples for each feature)\n        # Original: X.rowwise() -= mean (subtract mean from each row)\n        # Since our X_transposed has rows = features, we subtract mean across columns (samples)\n        mean_vector = np.mean(X_transposed, axis=1, keepdims=True)\n        X_centered = X_transposed - mean_vector\n        \n        # Compute covariance matrix of features\n        # Original: Cov = (X.adjoint() * X) / double(X.rows() - 1)\n        # X.adjoint() is conjugate transpose, but for real data it's just transpose\n        # X shape: (n_features, n_samples), X.adjoint() shape: (n_samples, n_features)\n        # So (X.adjoint() * X) gives (n_features, n_features) covariance matrix\n        # In numpy: X_centered @ X_centered.T gives same result\n        # Verify: X_centered shape (n_features, n_samples)\n        # X_centered.T shape (n_samples, n_features)\n        # X_centered @ X_centered.T shape (n_features, n_features) ✓\n        \n        # Numerical stability check: denominator for covariance\n        denominator = n_samples - 1\n        if abs(denominator) < 1e-8:\n            # Fallback: use identity-like matrix\n            Cov = np.eye(n_features_total)\n        else:\n            Cov = (X_centered @ X_centered.T) / denominator\n        \n        # Verify covariance matrix shape\n        # Should be (n_features_total, n_features_total)\n        assert Cov.shape == (n_features_total, n_features_total), \\\n            f\"Expected covariance shape ({n_features_total}, {n_features_total}), got {Cov.shape}\"\n        \n        # Eigen decomposition of symmetric covariance matrix\n        # Original uses SelfAdjointEigenSolver for symmetric matrices\n        # numpy.linalg.eigh is for symmetric/Hermitian matrices\n        evals, evecs = np.linalg.eigh(Cov)\n        \n        # Find eigenvector with largest eigenvalue\n        # Original: evals.maxCoeff(&idx_max)\n        idx_max = np.argmax(evals)\n        \n        # Get loadings (eigenvector corresponding to largest eigenvalue)\n        loadings = evecs[:, idx_max]  # shape: (n_features_total,)\n        \n        # Create pairs of (index, absolute loading value)\n        # Original uses 1-based indices for output, but we need 0-based for Python indexing\n        pairs = [(i, abs(loadings[i])) for i in range(len(loadings))]\n        \n        # Sort by absolute loading value in descending order\n        pairs.sort(key=lambda x: x[1], reverse=True)\n        \n        # Select top k features\n        k = min(n_features, len(pairs))\n        selected_indices = [pairs[i][0] for i in range(k)]\n        \n        # Return column names corresponding to selected indices\n        # CRITICAL: Must return column names, not indices\n        selected_features = [original_columns[i] for i in selected_indices]\n        \n        return selected_features",
      "created_at": "2025-12-11T16:15:55.448193",
      "updated_at": "2025-12-11T16:15:55.448193"
    }
  ],
  "modeling": [
    {
      "name": "Partial Least Squares Regression with CV Component Selection",
      "code": "import pandas as pd\nimport numpy as np\nfrom typing import Dict, Any, Tuple\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Data Format Assumptions:\n# - Input X: rows = samples, columns = features/wavelengths (DataFrame)\n# - Input y: Series with one target value per sample (same length as X)\n# - train() returns: dict containing trained model, best_ncomp, cv_scores, etc.\n# - predict() returns: np.ndarray (1D array, length = X.shape[0])\n#\n# Original Implementation Analysis:\n# - Original R code uses plsr() from pls package with CV validation\n# - It selects best number of components via minimum CV RMSE\n# - The original code reads pre-selected wavelength indices from file\n# - In our system, feature selection is done separately, so X input already contains selected features\n# - Original data layout: spectra_train had rows=wavelengths, columns=samples (transposed to rows=samples)\n# - Our system uses rows=samples, columns=features, so no transpose needed\n# - Mathematical operations: PLS regression, RMSE calculation, R² calculation\n# - Division operations: In R² calculation, denominator is sum((y - mean(y))^2) - need stability check\n# - Hyperparameter selection: which.min(cv_use) returns 1-based index, cv_use = cv_vals[-1] (excludes component 0)\n# - Python equivalent: best_ncomp = int(np.argmin(cv_scores)) + 1 (MANDATORY pattern)\n\nclass PLSRegressionModel(ModelingAlgorithm):\n    def get_name(self) -> str:\n        return \"Partial Least Squares Regression with CV Component Selection\"\n    \n    def get_params_info(self) -> Dict[str, Any]:\n        return {\n            \"max_components\": {\n                \"type\": \"int\",\n                \"default\": 15,\n                \"description\": \"Maximum number of PLS components to try during cross-validation\"\n            },\n            \"n_folds\": {\n                \"type\": \"int\",\n                \"default\": 10,\n                \"description\": \"Number of folds for cross-validation\"\n            },\n            \"random_seed\": {\n                \"type\": \"int\",\n                \"default\": 42,\n                \"description\": \"Random seed for reproducibility of CV splits\"\n            }\n        }\n    \n    def train(self, X: pd.DataFrame, y: pd.Series, params: Dict) -> Any:\n        \"\"\"\n        Train PLS regression model with cross-validation to select optimal number of components.\n        \n        Parameters:\n        -----------\n        X : pd.DataFrame\n            Training features, shape (n_samples, n_features)\n        y : pd.Series\n            Training target values, shape (n_samples,)\n        params : Dict\n            Algorithm parameters\n        \n        Returns:\n        --------\n        dict containing:\n            - 'model': trained PLSRegression model with best_ncomp components\n            - 'best_ncomp': optimal number of components (int)\n            - 'cv_scores': cross-validation RMSE scores for each component (list)\n            - 'feature_names': list of feature names from X.columns\n            - 'random_seed': random seed used\n        \"\"\"\n        # Extract parameters\n        max_components = params.get(\"max_components\", 15)\n        n_folds = params.get(\"n_folds\", 10)\n        random_seed = params.get(\"random_seed\", 42)\n        \n        # Store original feature names\n        feature_names = list(X.columns)\n        \n        # Convert to numpy arrays for sklearn\n        X_array = X.values\n        y_array = y.values\n        \n        # Initialize cross-validation\n        kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n        \n        # Store CV RMSE for each component (1 to max_components)\n        cv_scores = np.zeros(max_components)\n        \n        # Perform cross-validation for each component count\n        for n_comp in range(1, max_components + 1):\n            fold_scores = []\n            \n            for train_idx, val_idx in kf.split(X_array):\n                X_train_fold, X_val_fold = X_array[train_idx], X_array[val_idx]\n                y_train_fold, y_val_fold = y_array[train_idx], y_array[val_idx]\n                \n                # Train PLS model on fold\n                pls = PLSRegression(n_components=n_comp)\n                pls.fit(X_train_fold, y_train_fold)\n                \n                # Predict on validation fold\n                y_pred = pls.predict(X_val_fold).flatten()\n                \n                # Calculate RMSE for this fold\n                mse = mean_squared_error(y_val_fold, y_pred)\n                rmse = np.sqrt(mse)\n                fold_scores.append(rmse)\n            \n            # Average RMSE across folds for this component count\n            cv_scores[n_comp - 1] = np.mean(fold_scores)\n        \n        # Select best number of components (MANDATORY pattern)\n        # Note: cv_scores contains scores for components 1..max_components\n        # Original R code: cv_use = cv_vals[-1] (exclude component 0), then which.min(cv_use)\n        # In Python: best_ncomp = int(np.argmin(cv_scores)) + 1\n        best_ncomp = int(np.argmin(cv_scores)) + 1\n        \n        # Train final model with all training data using best_ncomp\n        final_model = PLSRegression(n_components=best_ncomp)\n        final_model.fit(X_array, y_array)\n        \n        # Calculate training performance metrics\n        y_pred_train = final_model.predict(X_array).flatten()\n        \n        # Calculate RMSE with numerical stability\n        mse_train = mean_squared_error(y_array, y_pred_train)\n        rmse_train = np.sqrt(mse_train)\n        \n        # Calculate R² with numerical stability check\n        y_mean = np.mean(y_array)\n        ss_total = np.sum((y_array - y_mean) ** 2)\n        \n        # MANDATORY: Division stability check\n        if abs(ss_total) < 1e-8:\n            r2_train = 0.0  # Safe fallback when denominator is near-zero\n        else:\n            ss_residual = np.sum((y_array - y_pred_train) ** 2)\n            r2_train = 1.0 - ss_residual / ss_total\n        \n        # Return model and metadata\n        return {\n            \"model\": final_model,\n            \"best_ncomp\": best_ncomp,\n            \"cv_scores\": cv_scores.tolist(),\n            \"feature_names\": feature_names,\n            \"training_metrics\": {\n                \"rmse\": rmse_train,\n                \"r2\": r2_train\n            },\n            \"random_seed\": random_seed,\n            \"max_components\": max_components,\n            \"n_folds\": n_folds\n        }\n    \n    def predict(self, model: Any, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Make predictions using trained PLS model.\n        \n        Parameters:\n        -----------\n        model : dict\n            Model object returned by train() method\n        X : pd.DataFrame\n            Prediction features, shape (n_samples, n_features)\n        \n        Returns:\n        --------\n        np.ndarray: Predicted values, shape (n_samples,)\n        \"\"\"\n        # Extract the trained PLS model\n        pls_model = model[\"model\"]\n        \n        # Ensure X has the same features as training (in same order)\n        # Note: In our system, feature selection ensures this, but we verify\n        expected_features = model.get(\"feature_names\", [])\n        if len(expected_features) > 0:\n            # Reorder columns to match training order if needed\n            if set(X.columns) == set(expected_features):\n                X = X[expected_features]\n        \n        # Convert to numpy array and make predictions\n        X_array = X.values\n        predictions = pls_model.predict(X_array).flatten()\n        \n        # CRITICAL: Return as 1D numpy array, length must match X.shape[0]\n        assert len(predictions) == X.shape[0], f\"Prediction length {len(predictions)} != samples {X.shape[0]}\"\n        return predictions",
      "created_at": "2025-12-11T16:22:32.380377",
      "updated_at": "2025-12-11T16:22:32.380377"
    },
    {
      "name": "PLS Regression with CV Component Selection",
      "code": "import pandas as pd\nimport numpy as np\nfrom typing import Dict, Any, List, Tuple\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.model_selection import KFold\nimport warnings\n\n# Data Format Assumptions:\n# - Input X: rows = samples, columns = features/wavelengths\n# - Input y: Series with one target value per sample (same length as X)\n# - Output train(): dict containing trained model and metadata\n# - Output predict(): np.ndarray (1D array, length = X.shape[0])\n#\n# Original Implementation:\n# - Original R code used PLS regression with cross-validation to select optimal number of components\n# - Original code transposed data (columns = samples, rows = features) but our system uses rows = samples\n# - Adapted by using sklearn PLSRegression which expects rows = samples, columns = features\n# - Preserved cross-validation logic and component selection method\n\nclass PLSRegressionModel(ModelingAlgorithm):\n    def get_name(self) -> str:\n        return \"PLS Regression with CV Component Selection\"\n    \n    def get_params_info(self) -> Dict[str, Any]:\n        return {\n            'max_components': {\n                'type': 'int',\n                'default': 15,\n                'description': 'Maximum number of PLS components to consider'\n            },\n            'cv_folds': {\n                'type': 'int',\n                'default': 10,\n                'description': 'Number of cross-validation folds'\n            },\n            'random_seed': {\n                'type': 'int',\n                'default': 42,\n                'description': 'Random seed for reproducibility'\n            }\n        }\n    \n    def train(self, X: pd.DataFrame, y: pd.Series, params: Dict) -> Any:\n        # Store original index and columns\n        original_index = X.index\n        original_columns = X.columns\n        \n        # Convert to numpy arrays for sklearn\n        X_array = X.values\n        y_array = y.values\n        \n        # Get parameters\n        max_components = params.get('max_components', 15)\n        cv_folds = params.get('cv_folds', 10)\n        random_seed = params.get('random_seed', 42)\n        \n        # Verify: X shape (n_samples, n_features), y shape (n_samples,)\n        n_samples, n_features = X_array.shape\n        assert len(y_array) == n_samples, f\"X has {n_samples} samples but y has {len(y_array)} values\"\n        \n        # Perform cross-validation to select optimal number of components\n        cv_scores = []\n        \n        # Create KFold object with random seed for reproducibility\n        kf = KFold(n_splits=cv_folds, shuffle=True, random_state=random_seed)\n        \n        # Test each number of components from 1 to max_components\n        for n_comp in range(1, max_components + 1):\n            fold_scores = []\n            \n            for train_idx, val_idx in kf.split(X_array):\n                X_train_fold = X_array[train_idx]\n                y_train_fold = y_array[train_idx]\n                X_val_fold = X_array[val_idx]\n                y_val_fold = y_array[val_idx]\n                \n                # Train PLS model on fold\n                pls = PLSRegression(n_components=n_comp)\n                pls.fit(X_train_fold, y_train_fold)\n                \n                # Predict on validation fold\n                y_pred_fold = pls.predict(X_val_fold).flatten()\n                \n                # Calculate RMSE for this fold\n                # Numerical stability check for division in RMSE calculation\n                n_val = len(y_val_fold)\n                if n_val > 0:\n                    mse = np.mean((y_val_fold - y_pred_fold) ** 2)\n                    # Check for negative mse due to numerical errors\n                    if mse < 0:\n                        mse = 0.0\n                    rmse = np.sqrt(mse)\n                    fold_scores.append(rmse)\n                else:\n                    fold_scores.append(np.inf)\n            \n            # Average RMSE across folds for this number of components\n            if len(fold_scores) > 0:\n                cv_scores.append(np.mean(fold_scores))\n            else:\n                cv_scores.append(np.inf)\n        \n        # Select optimal number of components\n        # CRITICAL: Use standard pattern: best_ncomp = int(np.argmin(cv_scores)) + 1\n        # cv_scores contains values for components 1 to max_components\n        # argmin returns 0-based index, so add 1 to get component number\n        if len(cv_scores) > 0:\n            best_ncomp_idx = int(np.argmin(cv_scores))\n            best_ncomp = best_ncomp_idx + 1\n        else:\n            best_ncomp = 1\n        \n        # Train final model with optimal number of components\n        final_model = PLSRegression(n_components=best_ncomp)\n        final_model.fit(X_array, y_array)\n        \n        # Calculate training performance metrics\n        y_pred_train = final_model.predict(X_array).flatten()\n        \n        # Calculate RMSE with numerical stability check\n        n_train = len(y_array)\n        if n_train > 0:\n            mse_train = np.mean((y_array - y_pred_train) ** 2)\n            if mse_train < 0:\n                mse_train = 0.0\n            rmse_train = np.sqrt(mse_train)\n            \n            # Calculate R² with numerical stability check\n            y_mean = np.mean(y_array)\n            ss_total = np.sum((y_array - y_mean) ** 2)\n            \n            # Check for near-zero denominator\n            if abs(ss_total) < 1e-8:\n                r2_train = 0.0\n            else:\n                ss_residual = np.sum((y_array - y_pred_train) ** 2)\n                r2_train = 1.0 - (ss_residual / ss_total)\n        else:\n            rmse_train = np.inf\n            r2_train = -np.inf\n        \n        # Return model and metadata\n        return {\n            'model': final_model,\n            'best_ncomp': best_ncomp,\n            'cv_scores': cv_scores,\n            'training_metrics': {\n                'rmse': rmse_train,\n                'r2': r2_train\n            },\n            'original_columns': original_columns.tolist(),\n            'original_index': original_index.tolist()\n        }\n    \n    def predict(self, model: Any, X: pd.DataFrame) -> np.ndarray:\n        # Extract the trained model from the dictionary\n        if isinstance(model, dict) and 'model' in model:\n            pls_model = model['model']\n            # Verify that X has the same features as training data\n            expected_columns = model.get('original_columns', [])\n            if len(expected_columns) > 0 and list(X.columns) != expected_columns:\n                warnings.warn(f\"X columns don't match training columns. Expected {len(expected_columns)} features, got {X.shape[1]}\")\n        else:\n            pls_model = model\n        \n        # Convert to numpy array for prediction\n        X_array = X.values\n        \n        # Make predictions\n        predictions = pls_model.predict(X_array)\n        \n        # Flatten to 1D array and ensure correct length\n        predictions_flat = predictions.flatten()\n        \n        # Verify output shape matches number of samples\n        assert len(predictions_flat) == X.shape[0], \\\n            f\"Predictions length {len(predictions_flat)} doesn't match X samples {X.shape[0]}\"\n        \n        return predictions_flat",
      "created_at": "2025-12-12T13:08:36.556334",
      "updated_at": "2025-12-12T13:19:17.992599"
    }
  ],
  "data_partitioning": [
    {
      "name": "Gasoline Data Partitioner",
      "code": "import pandas as pd\nimport numpy as np\nfrom typing import Dict, Any, Tuple\nfrom sklearn.model_selection import train_test_split\n\n# Data Format Assumptions:\n# - Input data: pd.DataFrame where rows = samples, columns = features + target\n#   * Last column is assumed to be the target variable\n#   * All other columns are features\n# - Output: Tuple of 4 objects:\n#   * X_train: DataFrame (rows=train_samples, columns=features)\n#   * X_test: DataFrame (rows=test_samples, columns=features)\n#   * y_train: Series (target values for train_samples)\n#   * y_test: Series (target values for test_samples)\n#\n# Original Implementation:\n# - Original code loaded separate files: spectra (wavelengths × samples) and ron (target)\n# - Original used train_test_split on sample indices, then selected columns from spectra\n# - Adapted to work with DataFrame where rows=samples, columns=features+target\n# - Preserves same split logic: test_size=0.2, random_state=42\n# - Note: Original code had shape check for wavelengths vs spectra rows, but in new format\n#   this check is not applicable since wavelengths are now feature columns\n\nclass GasolineDataPartitioner(DataPartitioningAlgorithm):\n    def get_name(self) -> str:\n        return \"Gasoline Data Partitioner\"\n    \n    def get_params_info(self) -> Dict[str, Any]:\n        return {\n            \"test_size\": {\n                \"type\": \"float\",\n                \"default\": 0.2,\n                \"description\": \"Proportion of samples to include in test split (0.0 to 1.0)\"\n            },\n            \"random_state\": {\n                \"type\": \"int\",\n                \"default\": 42,\n                \"description\": \"Random seed for reproducible splits\"\n            }\n        }\n    \n    def partition(self, data: pd.DataFrame, params: Dict) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n        # Extract parameters with defaults\n        test_size = params.get(\"test_size\", 0.2)\n        random_state = params.get(\"random_state\", 42)\n        \n        # Verify data is not empty\n        if data.empty:\n            raise ValueError(\"Input data is empty\")\n        \n        # Extract features (X) and target (y)\n        # Last column is target, all others are features\n        X = data.iloc[:, :-1]  # All columns except last\n        y = data.iloc[:, -1]   # Last column is target\n        \n        # Verify X and y have same number of samples\n        if len(X) != len(y):\n            raise ValueError(f\"X and y have different number of samples: X={len(X)}, y={len(y)}\")\n        \n        # Use train_test_split to partition data\n        # This matches the original algorithm's logic\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, \n            test_size=test_size, \n            random_state=random_state,\n            shuffle=True  # Default behavior in original code\n        )\n        \n        # Return as tuple of 4 objects\n        return X_train, X_test, y_train, y_test",
      "created_at": "2025-12-11T16:13:25.925958",
      "updated_at": "2025-12-11T16:13:25.925958"
    }
  ]
}